---
title: "Creating a Wordcloud"
author: "Spiro Stilianoudakis"
date: "11/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading Libraries

```{r}
#Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_231/")
library(openxlsx)
library(dplyr)
library(ggplot2)
library(stringr)
library(wordcloud)
library(wordcloud2)
library(tm)
library(textstem)
library(qdap)
```

# Reading in textdata

```{r}
comments <- read.xlsx("C:/Users/stili/Documents/Wordcloud/data/comments_dataset.xlsx",
                    sheet = "Sheet1",
                    startRow = 1,
                    colNames = TRUE,
                    na.strings = "",
                    detectDates = TRUE)

comments$score = comments$F2+comments$F3
comments$category <- ifelse(comments$score==0, "Neutral",
                            ifelse(comments$score<0, "Negative", "Positive"))
table(comments$category)
```

# All comments

```{r}
all_comments <- paste(comments$F1,collapse=" ")
all_comments_clean <- replace_contraction(all_comments)
all_comments_clean <- gsub("\\W", replace=" ", all_comments_clean) #remove punctuation
all_comments_clean <- gsub("\\d", replace=" ", all_comments_clean) #remove numbers 
all_comments_clean <- tolower(all_comments_clean) #create lower case string
all_comments_clean <- removeWords(all_comments_clean, stopwords()) #remove stopwords
all_comments_clean <- removeWords(all_comments_clean, c("get", "getting", "got", "just", "like", "can", "now", "im", "will")) #remove specific words
all_comments_clean <- gsub("\\b[A-z]\\b{1}", replace=" ", all_comments_clean) #remove words of lenght 1
all_comments_clean <- stripWhitespace(all_comments_clean) #remove extra white space

#Other cleaning techniques from qdap packaged
##bracketX(): Remove all text within brackets (e.g. “It’s (so) cool” becomes “It’s cool”)
##replace_number(): Replace numbers with their word equivalents (e.g. “2” becomes “two”)
##replace_abbreviation(): Replace abbreviations with their full text equivalents (e.g. “Sr” becomes “Senior”)
##replace_contraction(): Convert contractions back to their base words (e.g. “shouldn’t” becomes “should not”)
##replace_symbol() Replace common symbols with their word equivalents (e.g. “$” becomes “dollar”)

#plotting most frequent terms
frequent_terms <- freq_terms(all_comments_clean, 20)
plot(frequent_terms)

all_comments_clean <- str_split(all_comments_clean, pattern="\\s+")
all_comments_clean <- unlist(all_comments_clean)

#remove key words like braces, teeth, and invisalign
#all_comments_clean <- all_comments_clean[-which(all_comments_clean %in% c("braces", "teeth", "invisalign"))]

wordcloud(words = all_comments_clean,
          #freq=,
          max.words = 200,
          random.order = FALSE,
          min.freq = 2,
          colors = brewer.pal(6, "Dark2"),
          #colors = rainbow(3),
          use.r.layout = TRUE,
          scale = c(3,.5),
          rot.per = 0.3)

w <- data.frame(table(all_comments_clean))
colnames(w) <- c("word", "freq")
w<-w[order(w$freq,decreasing = TRUE),]

wordcloud2(w,
           size=.7,
           shape = "circle",
           rotateRatio = .3,
           minSize = 0)
```

## Comparison cloud

```{r}
#braces
all_comments_b <- paste(comments$F1[which(comments$group=="B")],collapse=" ")
all_comments_clean_b <- replace_contraction(all_comments_b)
all_comments_clean_b <- gsub("\\W", replace=" ", all_comments_clean_b) #remove punctuation
all_comments_clean_b <- gsub("\\d", replace=" ", all_comments_clean_b) #remove numbers 
all_comments_clean_b <- tolower(all_comments_clean_b) #create lower case string
all_comments_clean_b <- removeWords(all_comments_clean_b, stopwords()) #remove stopwords
all_comments_clean_b <- removeWords(all_comments_clean_b, c("get", "getting", "got", "just", "like", "can", "now", "im", "will")) #remove specific words
all_comments_clean_b <- gsub("\\b[A-z]\\b{1}", replace=" ", all_comments_clean_b) #remove words of lenght 1
all_comments_clean_b <- stripWhitespace(all_comments_clean_b) #remove extra white space

frequent_terms <- freq_terms(all_comments_clean_b, 20)
plot(frequent_terms)

all_comments_clean_b <- str_split(all_comments_clean_b, pattern="\\s+")
all_comments_clean_b <- unlist(all_comments_clean_b)

########################################################################

#direct to consumer aligners
all_comments_d <- paste(comments$F1[which(comments$group=="D")],collapse=" ")
all_comments_clean_d <- replace_contraction(all_comments_d)
all_comments_clean_d <- gsub("\\W", replace=" ", all_comments_clean_d) #remove punctuation
all_comments_clean_d <- gsub("\\d", replace=" ", all_comments_clean_d) #remove numbers 
all_comments_clean_d <- tolower(all_comments_clean_d) #create lower case string
all_comments_clean_d <- removeWords(all_comments_clean_d, stopwords()) #remove stopwords
all_comments_clean_d <- removeWords(all_comments_clean_d, c("get", "getting", "got", "just", "like", "can", "now", "im", "will")) #remove specific words
all_comments_clean_d <- gsub("\\b[A-z]\\b{1}", replace=" ", all_comments_clean_d) #remove words of lenght 1
all_comments_clean_d <- stripWhitespace(all_comments_clean_d) #remove extra white space

frequent_terms <- freq_terms(all_comments_clean_d, 20)
plot(frequent_terms)

all_comments_clean_d <- str_split(all_comments_clean_d, pattern="\\s+")
all_comments_clean_d <- unlist(all_comments_clean_d)

########################################################################

#in-office aligners
all_comments_i <- paste(comments$F1[which(comments$group=="I")],collapse=" ")
all_comments_clean_i <- replace_contraction(all_comments_i)
all_comments_clean_i <- gsub("\\W", replace=" ", all_comments_clean_i) #remove punctuation
all_comments_clean_i <- gsub("\\d", replace=" ", all_comments_clean_i) #remove numbers 
all_comments_clean_i <- tolower(all_comments_clean_i) #create lower case string
all_comments_clean_i <- removeWords(all_comments_clean_i, stopwords()) #remove stopwords
all_comments_clean_i <- removeWords(all_comments_clean_i, c("get", "getting", "got", "just", "like", "can", "now", "im", "will")) #remove specific words
all_comments_clean_i <- gsub("\\b[A-z]\\b{1}", replace=" ", all_comments_clean_i) #remove words of lenght 1
all_comments_clean_i <- stripWhitespace(all_comments_clean_i) #remove extra white space

frequent_terms <- freq_terms(all_comments_clean_i, 20)
plot(frequent_terms)

all_comments_clean_i <- str_split(all_comments_clean_i, pattern="\\s+")
all_comments_clean_i <- unlist(all_comments_clean_i)

###########################################################################

all_comments_clean_b = paste(all_comments_clean_b, collapse=" ")
all_comments_clean_d = paste(all_comments_clean_d, collapse=" ")
all_comments_clean_i = paste(all_comments_clean_i, collapse=" ")

# put everything in a single vector
all = c(all_comments_clean_b, 
        all_comments_clean_d,
        all_comments_clean_i)

# remove stop-words
#all = removeWords(all, c("braces", "teeth", "invisalign"))

# create corpus
corpus = Corpus(VectorSource(all))

# create term-document matrix
tdm = TermDocumentMatrix(corpus)

# convert as matrix
tdm = as.matrix(tdm)

# add column names
colnames(tdm) = c("Braces", "Direct Aligners", "In-office Aligners")

comparison.cloud(tdm, 
                 random.order=FALSE, 
                 colors = c("indianred3","lightsteelblue3", "forestgreen"),
                 title.colors = c("indianred3","lightsteelblue3", "forestgreen"),
                 title.size=2.5, 
                 scale = c(3,.5),
                 use.r.layout = TRUE,
                 max.words=200)
```

## Positive

### Braces

```{r}
pb <- comments[which(comments$category=="Positive" & comments$group=="B"),]
pb_comments <- paste(pb$F1,collapse=" ")
pb_comments_clean <- replace_contraction(pb_comments)
pb_comments_clean <- gsub("\\W", replace=" ", pb_comments_clean) #remove punctuation
pb_comments_clean <- gsub("\\d", replace=" ", pb_comments_clean) #remove numbers 
pb_comments_clean <- tolower(pb_comments_clean) #create lower case string
pb_comments_clean <- removeWords(pb_comments_clean, stopwords()) #remove stopwords
pb_comments_clean <- removeWords(pb_comments_clean, c("get", "getting", "got", "just", "like", "can", "now", "im", "will", "braces", "invisalign", "direct", "teeth")) #remove specific words
pb_comments_clean <- gsub("\\b[A-z]\\b{1}", replace=" ", pb_comments_clean) #remove words of lenght 1
pb_comments_clean <- stripWhitespace(pb_comments_clean) #remove extra white space

#plotting most frequent terms
#frequent_terms <- freq_terms(pb_comments_clean, 20)
#plot(frequent_terms)

pb_comments_clean <- str_split(pb_comments_clean, pattern="\\s+")
pb_comments_clean <- unlist(pb_comments_clean)

#remove key words like braces, teeth, and invisalign
#all_comments_clean <- all_comments_clean[-which(all_comments_clean %in% c("braces", "teeth", "invisalign"))]

wordcloud(words = pb_comments_clean,
          #freq=,
          max.words = 200,
          random.order = FALSE,
          min.freq = 2,
          colors = brewer.pal(6, "Dark2"),
          #colors = rainbow(3),
          use.r.layout = TRUE,
          scale = c(3,.5),
          rot.per = 0.3)

```

