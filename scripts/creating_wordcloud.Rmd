---
title: "Creating a Wordcloud"
author: "Spiro Stilianoudakis"
date: "11/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading Libraries

```{r}
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_231/")
library(openxlsx)
library(dplyr)
library(ggplot2)
library(stringr)
library(wordcloud)
library(wordcloud2)
library(tm)
library(textstem)
library(qdap)
```

# Reading in textdata

```{r}
comments <- read.xlsx("C:/Users/stili/Documents/Wordcloud/data/comments_dataset.xlsx",
                    sheet = "Sheet1",
                    startRow = 1,
                    colNames = TRUE,
                    na.strings = "",
                    detectDates = TRUE)

comments$score = comments$F2+comments$F3
comments$category <- ifelse(comments$score==0, "Neutral",
                            ifelse(comments$score<0, "Negative", "Positive"))
table(comments$category)

table(comments$group[-which(comments$category=="Neutral")], comments$category[-which(comments$category=="Neutral")])
chisq.test(table(comments$group[-which(comments$category=="Neutral")], comments$category[-which(comments$category=="Neutral")]))
prop.table(table(comments$group[-which(comments$category=="Neutral")], comments$category[-which(comments$category=="Neutral")]),1)

mean(comments$score[which(comments$category=="Positive")])
mean(comments$score[which(comments$category=="Negative")])
```

# All comments

```{r}
all_comments <- paste(comments$F1,collapse=" ")
all_comments_clean <- replace_contraction(all_comments)
all_comments_clean <- gsub("\\W", replace=" ", all_comments_clean) #remove punctuation
all_comments_clean <- gsub("\\d", replace=" ", all_comments_clean) #remove numbers 
all_comments_clean <- tolower(all_comments_clean) #create lower case string
all_comments_clean <- removeWords(all_comments_clean, stopwords()) #remove stopwords
all_comments_clean <- removeWords(all_comments_clean, c("get", "getting", "got", "just", "like", "can", "now", "im", "will")) #remove specific words
all_comments_clean <- gsub("\\b[A-z]\\b{1}", replace=" ", all_comments_clean) #remove words of lenght 1
all_comments_clean <- stripWhitespace(all_comments_clean) #remove extra white space

#Other cleaning techniques from qdap packaged
##bracketX(): Remove all text within brackets (e.g. “It’s (so) cool” becomes “It’s cool”)
##replace_number(): Replace numbers with their word equivalents (e.g. “2” becomes “two”)
##replace_abbreviation(): Replace abbreviations with their full text equivalents (e.g. “Sr” becomes “Senior”)
##replace_contraction(): Convert contractions back to their base words (e.g. “shouldn’t” becomes “should not”)
##replace_symbol() Replace common symbols with their word equivalents (e.g. “$” becomes “dollar”)

#plotting most frequent terms
frequent_terms <- freq_terms(all_comments_clean, 20)
plot(frequent_terms)

all_comments_clean <- str_split(all_comments_clean, pattern="\\s+")
all_comments_clean <- unlist(all_comments_clean)

#remove key words like braces, teeth, and invisalign
#all_comments_clean <- all_comments_clean[-which(all_comments_clean %in% c("braces", "teeth", "invisalign"))]

wordcloud(words = all_comments_clean,
          #freq=,
          max.words = 200,
          random.order = FALSE,
          min.freq = 2,
          colors = brewer.pal(6, "Dark2"),
          #colors = rainbow(3),
          use.r.layout = TRUE,
          scale = c(3,.5),
          rot.per = 0.3)

w <- data.frame(table(all_comments_clean))
colnames(w) <- c("word", "freq")
w<-w[order(w$freq,decreasing = TRUE),]

wordcloud2(w,
           size=.7,
           shape = "circle",
           rotateRatio = .3,
           minSize = 0)
```

# Function to create word cloud

```{r}
wordcloud_func <- function(wordData, subsetList, rmWords, freqTerms=TRUE, n=30, figPath=getwd(), maxWords = 200, minFreq = 2){
  
  #attach(wordData)
  
  if(class(wordData)!="data.frame"){print("wordData is not a data frame")}
  
  #filter data if necessary
  if(class(subsetList)=="list"){
    filt <- as.character()
    for(i in 1:length(subsetList)){
      newfilt <- noquote(paste0(noquote(paste0(names(subsetList)[i], "==")), "'", subsetList[[i]], "'"))
      filt <- c(filt, newfilt)
    }
    #filt <- noquote(paste(filt,collapse=","))
    filt <- noquote(paste(filt,collapse=" & "))
    wordData <- wordData %>%
      filter_(filt)
    #wordData <- wordData[which(filt),]
  }
  
  #cleaning
  wordData <- paste(wordData[,1],collapse=" ")
  wordData <- replace_contraction(wordData)
  wordData <- gsub("\\W", replace=" ", wordData) #remove punctuation
  wordData <- gsub("\\d", replace=" ", wordData) #remove numbers 
  wordData <- tolower(wordData) #create lower case string
  wordData <- removeWords(wordData, stopwords()) #remove stopwords
  wordData <- removeWords(wordData, rmWords) #remove specific words
  wordData <- gsub("\\b[A-z]\\b{1}", replace=" ", wordData) #remove words of lenght 1
  wordData <- stripWhitespace(wordData) #remove extra white space
  
  if(freqTerms){
    frequent_terms <- freq_terms(wordData, n)
    png(filename = paste(figPath, "/freqterms", "_", gsub("'","",filt), ".png", sep = ''),
        #res = 300,
        units = "px",
        width = 500,
        height = 500)
    plot(frequent_terms, main=paste("Top n =", n, "words"))
    dev.off()
  }
  
  wordData <- str_split(wordData, pattern="\\s+")
  wordData <- unlist(wordData)
  
  tiff(filename = paste(figPath, "/wordcloud", "_", gsub("'","",filt), ".png", sep = ''),
       width = 5, 
       height = 5, 
       units = 'in', 
       res = 300)
  wordcloud(words = wordData,
            #freq=,
            max.words = maxWords,
            random.order = FALSE,
            min.freq = minFreq,
            colors = brewer.pal(6, "Dark2"),
            #colors = rainbow(3),
            use.r.layout = TRUE,
            scale = c(3,.5),
            rot.per = 0.3)
  dev.off()
}


```

## Creating stratified wordclouds

```{r}
rmWords <- c("get", "getting", "got", "just", "can", "now", "im", "will", "braces", "invisalign", "direct", "teeth")

subsetList <- list(category="Positive")
wordcloud_func(wordData = comments, subsetList = subsetList, rmWords = rmWords)

subsetList <- list(category="Positive",group="B")
wordcloud_func(wordData = comments, subsetList = subsetList, rmWords = rmWords)

subsetList <- list(category="Positive",group="D")
wordcloud_func(wordData = comments, subsetList = subsetList, rmWords = rmWords)

subsetList <- list(category="Positive",group="I")
wordcloud_func(wordData = comments, subsetList = subsetList, rmWords = rmWords)

subsetList <- list(category="Negative")
wordcloud_func(wordData = comments, subsetList = subsetList, rmWords = rmWords)

subsetList <- list(category="Negative",group="B")
wordcloud_func(wordData = comments, subsetList = subsetList, rmWords = rmWords)

subsetList <- list(category="Negative",group="D")
wordcloud_func(wordData = comments, subsetList = subsetList, rmWords = rmWords)

subsetList <- list(category="Negative",group="I")
wordcloud_func(wordData = comments, subsetList = subsetList, rmWords = rmWords)
```

<!--
# Comparison cloud

```{r}
#braces
all_comments_b <- paste(comments$F1[which(comments$group=="B")],collapse=" ")
all_comments_clean_b <- replace_contraction(all_comments_b)
all_comments_clean_b <- gsub("\\W", replace=" ", all_comments_clean_b) #remove punctuation
all_comments_clean_b <- gsub("\\d", replace=" ", all_comments_clean_b) #remove numbers 
all_comments_clean_b <- tolower(all_comments_clean_b) #create lower case string
all_comments_clean_b <- removeWords(all_comments_clean_b, stopwords()) #remove stopwords
all_comments_clean_b <- removeWords(all_comments_clean_b, c("get", "getting", "got", "just", "like", "can", "now", "im", "will")) #remove specific words
all_comments_clean_b <- gsub("\\b[A-z]\\b{1}", replace=" ", all_comments_clean_b) #remove words of lenght 1
all_comments_clean_b <- stripWhitespace(all_comments_clean_b) #remove extra white space

frequent_terms <- freq_terms(all_comments_clean_b, 20)
plot(frequent_terms)

all_comments_clean_b <- str_split(all_comments_clean_b, pattern="\\s+")
all_comments_clean_b <- unlist(all_comments_clean_b)

########################################################################

#direct to consumer aligners
all_comments_d <- paste(comments$F1[which(comments$group=="D")],collapse=" ")
all_comments_clean_d <- replace_contraction(all_comments_d)
all_comments_clean_d <- gsub("\\W", replace=" ", all_comments_clean_d) #remove punctuation
all_comments_clean_d <- gsub("\\d", replace=" ", all_comments_clean_d) #remove numbers 
all_comments_clean_d <- tolower(all_comments_clean_d) #create lower case string
all_comments_clean_d <- removeWords(all_comments_clean_d, stopwords()) #remove stopwords
all_comments_clean_d <- removeWords(all_comments_clean_d, c("get", "getting", "got", "just", "like", "can", "now", "im", "will")) #remove specific words
all_comments_clean_d <- gsub("\\b[A-z]\\b{1}", replace=" ", all_comments_clean_d) #remove words of lenght 1
all_comments_clean_d <- stripWhitespace(all_comments_clean_d) #remove extra white space

frequent_terms <- freq_terms(all_comments_clean_d, 20)
plot(frequent_terms)

all_comments_clean_d <- str_split(all_comments_clean_d, pattern="\\s+")
all_comments_clean_d <- unlist(all_comments_clean_d)

########################################################################

#in-office aligners
all_comments_i <- paste(comments$F1[which(comments$group=="I")],collapse=" ")
all_comments_clean_i <- replace_contraction(all_comments_i)
all_comments_clean_i <- gsub("\\W", replace=" ", all_comments_clean_i) #remove punctuation
all_comments_clean_i <- gsub("\\d", replace=" ", all_comments_clean_i) #remove numbers 
all_comments_clean_i <- tolower(all_comments_clean_i) #create lower case string
all_comments_clean_i <- removeWords(all_comments_clean_i, stopwords()) #remove stopwords
all_comments_clean_i <- removeWords(all_comments_clean_i, c("get", "getting", "got", "just", "like", "can", "now", "im", "will")) #remove specific words
all_comments_clean_i <- gsub("\\b[A-z]\\b{1}", replace=" ", all_comments_clean_i) #remove words of lenght 1
all_comments_clean_i <- stripWhitespace(all_comments_clean_i) #remove extra white space

frequent_terms <- freq_terms(all_comments_clean_i, 20)
plot(frequent_terms)

all_comments_clean_i <- str_split(all_comments_clean_i, pattern="\\s+")
all_comments_clean_i <- unlist(all_comments_clean_i)

###########################################################################

all_comments_clean_b = paste(all_comments_clean_b, collapse=" ")
all_comments_clean_d = paste(all_comments_clean_d, collapse=" ")
all_comments_clean_i = paste(all_comments_clean_i, collapse=" ")

# put everything in a single vector
all = c(all_comments_clean_b, 
        all_comments_clean_d,
        all_comments_clean_i)

# remove stop-words
#all = removeWords(all, c("braces", "teeth", "invisalign"))

# create corpus
corpus = Corpus(VectorSource(all))

# create term-document matrix
tdm = TermDocumentMatrix(corpus)

# convert as matrix
tdm = as.matrix(tdm)

# add column names
colnames(tdm) = c("Braces", "Direct Aligners", "In-office Aligners")

comparison.cloud(tdm, 
                 random.order=FALSE, 
                 colors = c("indianred3","lightsteelblue3", "forestgreen"),
                 title.colors = c("indianred3","lightsteelblue3", "forestgreen"),
                 title.size=2.5, 
                 scale = c(3,.5),
                 use.r.layout = TRUE,
                 max.words=200)
```
-->

